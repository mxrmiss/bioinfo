#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æå–å¹¶æ¸…æ´— CDS + è›‹ç™½è´¨è„šæœ¬ï¼ˆCDS-driven + ç‰©ç§çº§å¹¶è¡Œ + è¶Šç•Œåæ ‡è¿‡æ»¤ï¼‰

æ›´æ–°è¦ç‚¹ï¼ˆæœ€å°å¿…è¦æ”¹åŠ¨ï¼‰ï¼š
1) å¹¶è¡Œï¼šå°†â€œæŒ‰ç‰©ç§å¤„ç†â€çš„ä¸»å¾ªç¯æ”¹ä¸ºå¤šè¿›ç¨‹æ± ï¼Œç‰©ç§ä¹‹é—´å¹¶è¡Œï¼›æ¯ä¸ªç‰©ç§ç‹¬å è‡ªå·±çš„ tmp å­ç›®å½•ã€‚
2) åæ ‡è¶Šç•Œä¿®å¤ï¼šåœ¨ gffread æå–å‰ï¼Œåˆ©ç”¨åŸºå› ç»„åºåˆ—é•¿åº¦è¿‡æ»¤ GFF ä¸­è¶Šç•Œ featureï¼ˆend>len æˆ– start<1ï¼‰ï¼Œ
   é¿å… gffread çš„ `GffObj::getSpliced() improper genomic coordinate` æŠ¥é”™ã€‚
3) ä»èµ°â€œCDSâ†’ç¿»è¯‘è›‹ç™½â€çš„è·¯çº¿ï¼šæœ€ç»ˆè›‹ç™½ç”±é€šè¿‡ QC çš„ CDS ç¿»è¯‘å¾—åˆ°ï¼Œä¿è¯ä¸€ä¸€å¯¹åº”ã€å¯å®¡è®¡ã€‚
4) å…¶ä½™æµç¨‹ã€é˜ˆå€¼ã€è¾“å‡ºè·¯å¾„ã€æ¸…ç†ç­–ç•¥ä¿æŒä¸å˜ã€‚
"""

import sys, re, shutil, gzip, subprocess, datetime, csv, os
from pathlib import Path
from typing import List, Optional, Dict, Tuple
from multiprocessing import Pool, cpu_count

# ========================== è·¯å¾„ä¸é˜ˆå€¼ï¼ˆå¯æŒ‰éœ€ä¿®æ”¹ï¼‰ ==========================
ANNOT_DIR = "data/annotation"          # GFF/GFF3 ç›®å½•
GENOME_DIR = "data/genomic"            # åŸºå› ç»„ FASTA ç›®å½•
OUT_PEP_DIR = "data/proteomes"         # è¾“å‡ºè›‹ç™½ç›®å½•
OUT_CDS_DIR = "data/cds"               # è¾“å‡ºCDSç›®å½•
TMP_DIR  = "data/.tmp_from_gff"        # ä¸­é—´æ–‡ä»¶ç›®å½•ï¼ˆæ¯ç‰©ç§ä¸€ä¸ªå­ç›®å½•ï¼‰
LIST_DIR = "results/proteomes"         # æ—¥å¿—/åˆ—è¡¨ç›®å½•
SPECIES_LIST = f"{LIST_DIR}/species.list"
MATCH_TSV = f"{LIST_DIR}/annot_genome_match.tsv"

SEQID_COVERAGE_MIN = 0.90              # GFFä¸FAåºåˆ—åé‡åˆç‡é˜ˆå€¼
RESUME_SKIP_PASS = True                # å·²é€šè¿‡ä¸”æœ‰è¾“å‡ºåˆ™è·³è¿‡

# -------- æ–°å¢æ€»å¼€å…³ï¼šç»“æŸæ—¶æ˜¯å¦åˆ é™¤ TMP ç›®å½•ï¼ˆé»˜è®¤ False=ä¸åˆ é™¤ï¼‰ --------
CLEANUP_TMP_AT_END = False

# ========================== ä¾èµ–å·¥å…·ï¼ˆAGATç°ä»£è„šæœ¬åï¼‰ ==========================
AGAT_KEEP_LONGEST = "agat_sp_keep_longest_isoform.pl"
AGAT_FIX_DUPS     = "agat_sp_fix_features_locations_duplicated.pl"
AGAT_FIX_PHASES   = "agat_sp_fix_cds_phases.pl"   # éœ€è¦ -g -f
GFFREAD_BIN       = "gffread"
AGAT_EXTRA_ARGS   = ""                  # é¢„ç•™é¢å¤–å‚æ•°ï¼ˆé»˜è®¤ç©ºï¼‰

# ========================== ID è§„èŒƒåŒ–ä¸QCå‚æ•° ==========================
STRIP_ID_PREFIXES = True
ID_PREFIX_REGEX   = r'^(rna\-|mrna\-|transcript:|cds:)'

PROT_MIN_LEN_AA   = 50                 # è›‹ç™½æœ€çŸ­é•¿åº¦ï¼ˆaaï¼‰
CDS_MIN_LEN_NT    = 150                # CDS æœ€çŸ­é•¿åº¦ï¼ˆntï¼Œä¸”éœ€3çš„å€æ•°ï¼‰

# ========================== å¹¶è¡Œå‚æ•° ==========================
# é»˜è®¤ä½¿ç”¨ CPU ä¸€åŠæ ¸å¿ƒï¼Œè‡³å°‘ 2ï¼›å¯ç”¨ç¯å¢ƒå˜é‡ N_WORKERS è¦†ç›–
N_WORKERS = max(2, int(os.environ.get("N_WORKERS", max(1, (cpu_count() or 4)//2))))

# é™åˆ¶ä¸€äº›åº•å±‚åº“çš„çº¿ç¨‹æ•°ï¼Œé¿å…è¿‡åº¦æŠ¢æ ¸ï¼ˆå¯¹å¤–éƒ¨å·¥å…·é€šå¸¸ä¹Ÿå®‰å…¨ï¼‰
os.environ.setdefault("OMP_NUM_THREADS", "1")
os.environ.setdefault("OPENBLAS_NUM_THREADS", "1")
os.environ.setdefault("MKL_NUM_THREADS", "1")
os.environ.setdefault("VECLIB_MAXIMUM_THREADS", "1")
os.environ.setdefault("NUMEXPR_NUM_THREADS", "1")

# ========================== æ‰©å±•åä¸å·¥å…·æ£€æµ‹ ==========================
GFF_EXTS = [".gff.gz", ".gff3.gz", ".gff", ".gff3"]
FA_EXTS  = [".fa.gz", ".fna.gz", ".fasta.gz", ".fa", ".fna", ".fasta"]
TOOLS = ["awk", "gzip", "seqkit", GFFREAD_BIN, AGAT_KEEP_LONGEST, AGAT_FIX_DUPS, AGAT_FIX_PHASES]

MATCH_HEADER = ["species","gff_file","genome_file","cov_frac","status","pep_kept","cds_kept","note","timestamp"]

# ========================== è¿è¡Œè¾…åŠ©å‡½æ•° ==========================
def need(cmd: str):
    """æ£€æŸ¥ä¾èµ–å‘½ä»¤æ˜¯å¦å­˜åœ¨ï¼›ä¸å­˜åœ¨åˆ™æŠ›é”™"""
    if subprocess.call(f"command -v {cmd} >/dev/null 2>&1", shell=True) != 0:
        raise RuntimeError(f"[ERR] dependency not found: {cmd}")

def ensure_dir(p: Path): p.mkdir(parents=True, exist_ok=True)
def is_gz(path: Path) -> bool: return str(path).endswith(".gz")

def run(cmd: str, label: str = "CMD"):
    """æ‰§è¡Œ shell å‘½ä»¤ï¼›é€ä¼ è¾“å‡ºï¼›å¤±è´¥åˆ™æŠ›å‡ºå¼‚å¸¸ï¼ˆä¾¿äºå¹¶è¡Œä¸‹å•ç‰©ç§å¤±è´¥ä¸å½±å“å…¨å±€ï¼‰"""
    print(f"[{label}] {cmd}")
    res = subprocess.run(cmd, shell=True)
    if res.returncode != 0:
        raise RuntimeError(f"[ERR] command failed ({label}), exit code: {res.returncode}")

def bash_out(cmd: str) -> str:
    """è·å–å‘½ä»¤stdoutæ–‡æœ¬ï¼ˆstderré™é»˜ï¼‰"""
    p = subprocess.run(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.DEVNULL, text=True)
    return p.stdout or ""

def _cat(path: Path) -> str:
    """æ ¹æ®æ˜¯å¦å‹ç¼©é€‰æ‹© cat æˆ– gzip -cd"""
    return f"gzip -cd '{path}'" if is_gz(path) else f"cat '{path}'"

def safe_int_last_line(s: str) -> int:
    s = (s or "").strip()
    if not s: return 0
    try:
        return int(s.splitlines()[-1].strip())
    except Exception:
        digits = re.findall(r"\d+", s)
        return int(digits[-1]) if digits else 0

def count_fasta_records(p: Path) -> int:
    return safe_int_last_line(bash_out(f"test -f '{p}' && grep -c '^>' '{p}' 2>/dev/null || echo 0"))

# ========================== ç‰©ç§ä¸è·¯å¾„ ==========================
def list_species_from_annotation(annot_dir: Path) -> List[str]:
    """ä» annotation/ ä¸­åˆ—å‡ºç‰©ç§stemï¼ˆå»é™¤å„ç§ä¸­é—´åç¼€ï¼‰"""
    stems_set = set()
    for f in sorted(annot_dir.iterdir()):
        if not f.is_file(): continue
        m = re.search(r'^(?P<stem>.+?)\.(gff3?|gff)(\.gz)?$', f.name)
        if not m: continue
        stem = m.group("stem")
        stem = re.sub(r'\.(agat_longest|keep_longest|agat.*|clean4agat|clean(?:ed)?|filtered.*|fixloc|fixphase)$', '', stem)
        stems_set.add(stem)
    return sorted(stems_set)

def find_by_stem(stem: str, directory: Path, exts: List[str]) -> Optional[Path]:
    """æŒ‰ stem + æ‰©å±•ååœ¨ç›®å½•ä¸­æŸ¥æ‰¾æ–‡ä»¶"""
    for e in exts:
        f = directory / f"{stem}{e}"
        if f.is_file(): return f
    return None

# ========================== è¦†ç›–åº¦æ£€æŸ¥ ==========================
def coverage_details(gff: Path, fa: Path) -> Tuple[float, int, int, int]:
    """
    è®¡ç®— GFF çš„ seqid é›†åˆä¸ FASTA çš„åºåˆ—åé›†åˆçš„é‡åˆæ¯”ä¾‹
    è¿”å›ï¼šè¦†ç›–ç‡ã€GFFåºåˆ—æ•°ã€FASTAåºåˆ—æ•°ã€é‡å æ•°
    """
    env_prefix = "LC_ALL=C"
    gff_ids = bash_out(f"{env_prefix} " + _cat(gff) + r" | awk -F'\t' '$0!~/^#/ && NF>=1 {print $1}' | sort -u").splitlines()
    fa_ids  = bash_out(f"{env_prefix} " + _cat(fa)  + r" | grep '^>' | sed 's/^>//;s/ .*//' | sort -u").splitlines()
    if not gff_ids or not fa_ids:
        return 0.0, len(set(gff_ids)), len(set(fa_ids)), 0
    s_g = set(gff_ids); s_f = set(fa_ids)
    overlap = len(s_g & s_f)
    cov = overlap / len(s_g) if s_g else 0.0
    return cov, len(s_g), len(s_f), overlap

# ========================== ç¼“å­˜è¯»å†™ï¼ˆmatchè¡¨ï¼‰ ==========================
def load_match_table(path: Path) -> Dict[str, Dict[str, str]]:
    if not path.is_file(): return {}
    data: Dict[str, Dict[str, str]] = {}
    with path.open("r", encoding="utf-8") as f:
        rdr = csv.DictReader(f, delimiter="\t")
        for row in rdr:
            sp = row.get("species","").strip()
            if sp: data[sp] = row
    return data

def write_match_table(path: Path, rows: List[Dict[str, str]]):
    ensure_dir(path.parent)
    with path.open("w", encoding="utf-8") as f:
        w = csv.DictWriter(f, fieldnames=MATCH_HEADER, delimiter="\t", lineterminator="\n")
        w.writeheader()
        for r in rows: w.writerow(r)

# ========================== IDè§„èŒƒåŒ–ä¸QC ==========================
def norm_ids_file(p: Path):
    """å¯¹ä¸€ä¸ªåªå« ID çš„æ–‡ä»¶è¿›è¡Œå‰ç¼€æ¸…ç†ï¼ˆæ¯”å¦‚ rna-/mrna-/transcript: ç­‰ï¼‰"""
    if STRIP_ID_PREFIXES:
        run(f"awk '{{gsub(/^{ID_PREFIX_REGEX}/,\"\"); print}}' '{p}' > '{p}.norm' && mv '{p}.norm' '{p}'", "NORM")

def clean_cds_codon_safe(fna_in: Path, fna_out: Path, keep_ids_out: Path) -> int:
    """
    å¯¹ CDS åšâ€œå¯†ç å­å®‰å…¨æ€§â€ç­›é€‰ï¼ˆCDS-drivenå…³é”®æ­¥éª¤ï¼‰ï¼š
    - é•¿åº¦ â‰¥ CDS_MIN_LEN_NT ä¸” é•¿åº¦%3==0ï¼ˆç›´æ¥å¯¹åºåˆ—è®¡ç®— lengthï¼‰
    - ç¿»è¯‘åä¸å…è®¸å†…éƒ¨ '*'ï¼ˆå…è®¸æœ«ç«¯ 1 ä¸ª '*'ï¼‰
    - è¾“å‡ºä¿ç•™ ID åˆ—è¡¨ keep.idsï¼Œå¹¶æŒ‰è¯¥åˆ—è¡¨åº”ç”¨åˆ° CDSï¼ˆå¾—åˆ°æœ€ç»ˆ codon-safe CDSï¼‰
    æ³¨ï¼šfx2tab ä½¿ç”¨ -i -sï¼ˆid, seqï¼‰ï¼Œé¿å…åˆ—ä½æ¬¡æ­§ä¹‰
    """
    tmp_len = Path(str(fna_out) + ".len3.tmp")
    # ä½¿ç”¨ -i -sï¼š$1=id, $2=seq
    run(
        f"seqkit fx2tab -i -s '{fna_in}' | "
        f"awk '{{ if(length($2)>={CDS_MIN_LEN_NT} && length($2)%3==0) print \">\"$1\"\\n\"$2 }}' | "
        f"seqkit seq -w 0 > '{tmp_len}'",
        "QC:CDS_LEN_MOD3"
    )
    pep_tmp = Path(str(fna_out) + ".pep.tmp")
    run(f"seqkit translate -w 0 '{tmp_len}' > '{pep_tmp}'", "QC:TRANSLATE")

    # ç»Ÿè®¡å†…éƒ¨ç»ˆæ­¢ï¼šå…è®¸å°¾éƒ¨1ä¸ª'*'ï¼Œå†…éƒ¨ä¸å…è®¸
    awk_stop = r"""{
      id=$1; pep=$2; t=pep; gsub(/\*/,"",t); sc=length(pep)-length(t);
      if (sc==0 || (sc==1 && substr(pep,length(pep),1)=="*")) print id;
    }"""
    run(f"seqkit fx2tab -i -s '{pep_tmp}' | awk '{awk_stop}' > '{keep_ids_out}'", "QC:KEEP_IDS")

    # è§„èŒƒåŒ– keep.idsï¼ˆä¸€æ¬¡å³å¯ï¼‰
    norm_ids_file(keep_ids_out)

    # åº”ç”¨ keep.ids åˆ° CDSï¼ˆå¾—åˆ°æœ€ç»ˆ CDSï¼‰
    n_ids = safe_int_last_line(bash_out(f"test -s '{keep_ids_out}' && wc -l < '{keep_ids_out}' || echo 0"))
    if n_ids > 0:
        run(f"seqkit grep -f '{keep_ids_out}' '{tmp_len}' | seqkit seq -w 0 > '{fna_out}'", "QC:APPLY_IDS")
    else:
        Path(fna_out).write_text("", encoding="utf-8")

    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    for p in [tmp_len, pep_tmp]:
        try: Path(p).unlink()
        except Exception: pass

    return count_fasta_records(fna_out)

def translate_cds_to_protein(final_cds: Path, out_pep: Path) -> int:
    """
    ç”±â€œæœ€ç»ˆ CDSâ€ç›´æ¥ç¿»è¯‘å¾—åˆ°è›‹ç™½ï¼ˆCDS-driven çš„äº§ç‰©ï¼‰å¹¶åšè½»åº¦ QCï¼š
    - å»æ‰å”¯ä¸€çš„æœ«ç«¯ '*'
    - ä¸å…è®¸å†…éƒ¨ '*'
    - éå¸¸è§æ°¨åŸºé…¸ B/Z/J/U/O â†’ 'X'
    - è¿‡æ»¤æ‰é•¿åº¦ < PROT_MIN_LEN_AA
    """
    pep_raw = Path(str(out_pep) + ".raw.tmp")
    run(f"seqkit translate -w 0 '{final_cds}' > '{pep_raw}'", "PROT:TRANSLATE_FROM_CDS")

    awk_script = r"""{
      id=$1; seq=toupper($2);
      gsub(/B|Z|J|U|O/,"X",seq);                # éå¸¸è§æ°¨åŸºé…¸ â†’ X
      t=seq; gsub(/\*$/,"",t); seq=t;           # å»æ‰æœ«ç«¯å”¯ä¸€ '*'
      t2=seq; gsub(/\*/,"",t2); sc=length(seq)-length(t2);
      if (sc>0) next;                           # å†…éƒ¨ '*' â†’ ä¸¢å¼ƒï¼ˆç†è®ºä¸Šä¸ä¼šï¼Œå› ä¸ºå‰é¢å·²ç­›ï¼‰
      if (length(seq)>0 && length(seq)<%MINLEN%) next;
      print ">" id "\n" seq
    }""".replace("%MINLEN%", str(PROT_MIN_LEN_AA))

    pep_qc = Path(str(out_pep) + ".qc.tmp")
    run(f"seqkit fx2tab -i -s '{pep_raw}' | awk '{awk_script}' | seqkit seq -w 0 > '{pep_qc}'", "PROT:POST_QC")

    npep = count_fasta_records(pep_qc)
    if npep > 0:
        shutil.move(pep_qc, out_pep)
    else:
        Path(out_pep).write_text("", encoding="utf-8")
        try: pep_qc.unlink()
        except Exception: pass

    try: pep_raw.unlink()
    except Exception: pass

    return npep

# ========================== è¶Šç•Œåæ ‡è¿‡æ»¤ï¼ˆå…³é”®ä¿®å¤ï¼‰ ==========================
def filter_gff_by_contig_length(gff_in: Path, fa_plain: Path, gff_out: Path, report_out: Path) -> Tuple[int,int]:
    """
    ç”ŸæˆåŸºå› ç»„ contig é•¿åº¦è¡¨ï¼Œç„¶åè¿‡æ»¤ GFF ä¸­ä»»ä½•è¶…å‡ºé•¿åº¦èŒƒå›´çš„ feature è¡Œï¼š
    - å…è®¸æ³¨é‡Šè¡ŒåŸæ ·é€šè¿‡
    - ä»…å¯¹å…·æœ‰ 9 åˆ—çš„ feature è¡Œæ£€æŸ¥ $4(start)ã€$5(end)
    - æ¡ä»¶ï¼š1 <= start <= end <= len[seqid]
    - è¢«å‰”é™¤çš„è¡Œè®¡æ•°å†™å…¥æŠ¥å‘Š
    è¿”å› (kept, removed)
    """
    ensure_dir(gff_out.parent)
    contig_len = gff_out.parent / "contig.len.tsv"
    run(f"seqkit fx2tab -n -l '{fa_plain}' > '{contig_len}'", "LEN:CONTIG_TABLE")

    # è¿‡æ»¤å¹¶ç»Ÿè®¡
    awk_filter = r"""BEGIN{FS=OFS="\t"}
NR==FNR {len[$1]=$2; next}
$0 ~ /^#/ {print; next}
NF<9 {next}
{
  seq=$1; s=$4; e=$5;
  if (seq in len && s>=1 && e>=s && e<=len[seq]) {print; kept++} else {removed++}
}
END{
  if (ENVIRON["__GFF_LEN_REPORT"]!="") {
    print "kept=" kept "\tremoved=" removed > ENVIRON["__GFF_LEN_REPORT"];
  }
}"""
    # è®¾ç½®ç¯å¢ƒå˜é‡æŠŠç»Ÿè®¡å†™åˆ°æŠ¥å‘Šæ–‡ä»¶
    env = os.environ.copy()
    env["__GFF_LEN_REPORT"] = str(report_out)
    print("[CLEAN:LENCHECK] filtering out-of-bound features by contig lengths ...")
    res = subprocess.run(
        f"awk '{awk_filter}' '{contig_len}' '{gff_in}' > '{gff_out}'",
        shell=True, env=env
    )
    if res.returncode != 0:
        raise RuntimeError("[ERR] GFF length-based filtering failed")

    # è¯»å–æŠ¥å‘Šæ•°å­—
    kept, removed = 0, 0
    if report_out.is_file():
        txt = report_out.read_text().strip()
        m1 = re.search(r'kept=(\d+)', txt); m2 = re.search(r'removed=(\d+)', txt)
        kept = int(m1.group(1)) if m1 else 0
        removed = int(m2.group(1)) if m2 else 0
    return kept, removed

# ç»Ÿè®¡ GFF ä¸­ CDS è¡Œæ•°ï¼ˆç”¨äºè¿‡æ»¤å‰åå¯¹æ¯” / å›é€€åˆ¤æ–­ï¼‰
def _count_cds_lines(p: Path) -> int:
    return safe_int_last_line(bash_out(f"awk -F'\t' '$3==\"CDS\"{{c++}} END{{print c+0}}' '{p}'"))

# ========================== å•ç‰©ç§å¤„ç†ï¼ˆä¾›å¹¶è¡Œè°ƒç”¨ï¼‰ ==========================
def process_one_species(args) -> Dict[str, str]:
    """
    ç‹¬ç«‹å¤„ç†ä¸€ä¸ªç‰©ç§ï¼šAGATä¿®å¤ â†’ è¿‡æ»¤ â†’ æå–CDS â†’ QC â†’ ç¿»è¯‘è›‹ç™½
    è¿”å›å†™ match è¡¨æ‰€éœ€çš„è¡Œï¼ˆå­—å…¸ï¼‰ï¼Œå¤±è´¥åˆ™æŠ›å¼‚å¸¸ï¼ˆä¸»è¿›ç¨‹æ•è·å¹¶æ±‡æ€»ï¼‰
    """
    sp, gff_path, fa_path, cache_row = args
    sp_tmp = Path(TMP_DIR)/sp
    ensure_dir(sp_tmp)

    gff_plain = sp_tmp / "annot.gff3"
    fa_plain  = sp_tmp / "genome.fna"

    # è§£å‹/å¤åˆ¶åˆ°å·¥ä½œåŒº
    if str(gff_path).endswith(".gz"):
        run(f"gzip -cd '{gff_path}' > '{gff_plain}'", f"PREP:GUNZIP_GFF ({sp})")
    else:
        run(f"cp -f '{gff_path}' '{gff_plain}'", f"PREP:CP_GFF ({sp})")

    if str(fa_path).endswith(".gz"):
        run(f"gzip -cd '{fa_path}' > '{fa_plain}'", f"PREP:GUNZIP_FA ({sp})")
    else:
        run(f"cp -f '{fa_path}' '{fa_plain}'", f"PREP:CP_FA ({sp})")

    # 0) ä¿®å¤é‡å¤ä½ç½®
    fixed1 = sp_tmp / "fixloc.gff3"
    run(f"{AGAT_FIX_DUPS} -g '{gff_plain}' -o '{fixed1}'", "AGAT:FIX_DUPS (1/3)")

    # 1) ä¿®å¤ phaseï¼ˆéœ€ genomeï¼‰
    fixed2 = sp_tmp / "fixphase.gff3"
    run(f"{AGAT_FIX_PHASES} -g '{fixed1}' -f '{fa_plain}' -o '{fixed2}'", "AGAT:FIX_PHASE (2/3)")
    if (not fixed2.exists()) or fixed2.stat().st_size < 1024:
        print(f"[WARN] {sp}: fixphase æ–‡ä»¶ç¼ºå¤±æˆ–è¿‡å°ï¼Œå›é€€ä½¿ç”¨ fixloc ç»“æœã€‚")
        fixed2 = fixed1

    # 2) ä»…ä¿ç•™å¿…è¦ feature â€”â€” æ¢å¤ gene
    gff_clean = sp_tmp / "clean4agat.gff3"
    run(
        "awk -F'\\t' 'BEGIN{OFS=\"\\t\"} "
        "/^#/ {print; next} "
        "{t=$3; if (t==\"gene\"||t==\"mRNA\"||t==\"transcript\"||t==\"exon\"||t==\"CDS\"||"
        "t==\"five_prime_UTR\"||t==\"three_prime_UTR\"||t==\"start_codon\"||t==\"stop_codon\") print;}' "
        f"'{fixed2}' > '{gff_clean}'",
        "CLEAN:GFF_FILTER"
    )

    # 3) æ¯åŸºå› ä¿ç•™æœ€é•¿è½¬å½•æœ¬
    gff_agat = sp_tmp / "agat_longest.gff3"
    run(f"{AGAT_KEEP_LONGEST} -g '{gff_clean}' -o '{gff_agat}' {AGAT_EXTRA_ARGS}", "AGAT:KEEP_LONGEST (3/3)")

    # --- å¯é€‰ç»Ÿè®¡ï¼škeep_longest å‰å CDS è¡Œæ•°ï¼ˆä¾¿äºæ’é”™ï¼‰
    cds_before = _count_cds_lines(gff_clean)
    cds_after  = _count_cds_lines(gff_agat)
    print(f"[STAT] {sp}: CDS lines before/after keep_longest = {cds_before}/{cds_after}")

    # â˜… 3.5) åŸºäº contig é•¿åº¦çš„è¶Šç•Œè¿‡æ»¤ï¼ˆå…³é”®ä¿®å¤ï¼‰
    gff_lenf = sp_tmp / "lenfilter.gff3"
    report   = sp_tmp / "lenfilter.report.txt"
    kept, removed = filter_gff_by_contig_length(gff_agat, fa_plain, gff_lenf, report)
    if removed > 0:
        print(f"[CLEAN:LENCHECK] {sp}: removed {removed} out-of-bound features; kept {kept}.")

    # â€”â€” æ–°å¢ï¼šè‹¥é•¿åº¦è¿‡æ»¤å CDS ä¸º 0ï¼Œåˆ™å›é€€ï¼ˆé¿å… seqid ä¸åŒ¹é…â€œå›¢ç­â€ï¼‰
    cds_after_lenf = _count_cds_lines(gff_lenf)
    print(f"[STAT] {sp}: CDS lines after len-filter = {cds_after_lenf}")
    if cds_after_lenf == 0:
        print(f"[WARN] {sp}: len-filter produced 0 CDS; fallback to keep_longest output.")
        gff_lenf = gff_agat

    # 4) ä»…æå– CDSï¼ˆä¸å†æè›‹ç™½ï¼‰ â€”â€” å»æ‰ -Vï¼Œä¿ç•™ -E
    all_cds = sp_tmp / "all.cds.fna"
    run(f"{GFFREAD_BIN} -E '{gff_lenf}' -g '{fa_plain}' -x '{all_cds}'", "GFFREAD:EXTRACT_CDS")

    # 4.5) æ–­ç‚¹ç»Ÿè®¡ï¼šgffread æå–æ¡æ•°
    n_all_cds = count_fasta_records(all_cds)
    print(f"[STAT] {sp}: gffread CDS extracted = {n_all_cds}")
    if n_all_cds == 0:
        raise RuntimeError(f"[FATAL] {sp}: gffread produced 0 CDS (check gene/mRNA hierarchy).")

    # 5) è§„èŒƒIDå¹¶å†™ä¸­é—´CDSï¼ˆå†åšQCï¼‰
    out_cds = Path(OUT_CDS_DIR)  / f"{sp}.fna"
    ensure_dir(out_cds.parent)
    if STRIP_ID_PREFIXES:
        run(
            f"seqkit replace -p '^\\s*(\\S+).*' -r '$1' '{all_cds}' | "
            f"seqkit replace -p '{ID_PREFIX_REGEX}' -r '' | seqkit seq -w 0 > '{out_cds}'",
            "CDS:NORM_ID"
        )
    else:
        run(f"seqkit replace -p '^\\s*(\\S+).*' -r '$1' '{all_cds}' | seqkit seq -w 0 > '{out_cds}'", "CDS:NORM_ID")

    # 6) å¯†ç å­å®‰å…¨æ€§ QCï¼ˆkeep.ids äº§ç”Ÿäºæ­¤ï¼Œå¾—åˆ°æœ€ç»ˆ CDSï¼‰
    keep_ids = sp_tmp / "keep.ids"
    n_cds = clean_cds_codon_safe(out_cds, out_cds, keep_ids)
    if n_cds == 0:
        raise RuntimeError(f"[FATAL] {sp}: codon-safe CDS count is 0.")

    # 7) ç”±â€œæœ€ç»ˆ CDSâ€ç›´æ¥ç¿»è¯‘ç”Ÿæˆâ€œæœ€ç»ˆè›‹ç™½â€ï¼ˆCDS-drivenï¼‰
    out_pep = Path(OUT_PEP_DIR) / f"{sp}.faa"
    ensure_dir(out_pep.parent)
    npep = translate_cds_to_protein(out_cds, out_pep)
    if npep == 0:
        raise RuntimeError(f"[FATAL] {sp}: translated protein count is 0 after QC.")

    # 8) æ¸…ç†ç‰©ç§ä¸´æ—¶æ–‡ä»¶ï¼ˆä¸è§¦ç¢° annotation/ï¼‰
    for patt in ["*.fna","*.fa","*.fasta","*.cds.fna","*.protein.faa","*.gtf","*.tsv",
                 "keep*.ids","*.pep.tmp","*.len3.tmp","*.tmp","tmp.translate.faa",
                 "*.agat_longest.gff3","*.clean4agat.gff3","*.fixloc.gff3","*.fixphase.gff3",
                 "contig.len.tsv","lenfilter.report.txt","lenfilter.gff3"]:
        for f in sp_tmp.glob(patt):
            try: f.unlink()
            except Exception: pass

    # è¿”å›ç»™ä¸»è¿›ç¨‹å†™ match è¡¨
    row = {
        "species": sp,
        "gff_file": cache_row.get("gff_file", str(gff_path)),
        "genome_file": cache_row.get("genome_file", str(fa_path)),
        "cov_frac": cache_row.get("cov_frac","0.0"),
        "status": "PASS_CODON_SAFE",
        "pep_kept": str(npep),
        "cds_kept": str(n_cds),
        "note": "",
        "timestamp": datetime.datetime.now().isoformat(timespec="seconds")
    }
    print(f"[DONE] {sp}: codon-safe CDS={n_cds}, PEP={npep}")
    return row

# ========================== é¡¶å±‚ workerï¼ˆä¸ºäº†è§£å†³ pickling é—®é¢˜ï¼‰ ==========================
def _worker(argtuple):
    """
    è¿›ç¨‹æ± è°ƒç”¨çš„é¡¶å±‚å‡½æ•°ï¼ˆå¿…é¡»åœ¨æ¨¡å—é¡¶å±‚ï¼Œä¸èƒ½åµŒåœ¨ main()ï¼‰
    è¿”å› ("OK", row) æˆ– ("ERR", {"species": sp, "error": msg})
    """
    try:
        return ("OK", process_one_species(argtuple))
    except Exception as e:
        sp = argtuple[0] if isinstance(argtuple, (list, tuple)) and argtuple else "?"
        return ("ERR", {"species": sp, "error": str(e)})

# ========================== ä¸»æµç¨‹ ==========================
def main():
    os.environ.setdefault("LC_ALL", "C")

    # ä¾èµ–æ£€æŸ¥ & ç›®å½•å‡†å¤‡
    for t in TOOLS: need(t)
    for d in [OUT_PEP_DIR, OUT_CDS_DIR, TMP_DIR, LIST_DIR]: ensure_dir(Path(d))

    print("[STEP] å¯åŠ¨å‰æ¸…ç†ä¸´æ—¶ç›®å½• ...")
    if Path(TMP_DIR).exists():
        try:
            shutil.rmtree(TMP_DIR, ignore_errors=True)
            print(f"[CLEAN] removed {TMP_DIR}")
        except Exception as e:
            print(f"[WARN] failed to remove old tmp: {e}")

    annot = Path(ANNOT_DIR); genome = Path(GENOME_DIR)
    species = list_species_from_annotation(annot)
    if not species:
        print("[ERR] annotation ç›®å½•ä¸­æœªå‘ç° GFF/GFF3ã€‚", file=sys.stderr); sys.exit(1)

    match_path = Path(MATCH_TSV)
    cache = load_match_table(match_path)
    rows_out: Dict[str, Dict[str, str]] = {sp: cache[sp] for sp in cache}

    print("[CHECK] è¦†ç›–åº¦æ£€æŸ¥ï¼ˆä¸¥æ ¼æ¨¡å¼ï¼šä»»ä¸€å¤±è´¥â†’é€€å‡ºï¼‰ ...")
    pairs = []
    for sp in species:
        gff = find_by_stem(sp, annot, GFF_EXTS)
        fa  = find_by_stem(sp, genome, FA_EXTS)
        if not gff or not fa:
            rows_out[sp] = {"species":sp,"gff_file":str(gff or ""), "genome_file":str(fa or ""),"cov_frac":"0.0","status":"FAIL_NO_FILE","pep_kept":"0","cds_kept":"0","note":"missing GFF or Genome","timestamp":datetime.datetime.now().isoformat(timespec="seconds")}
            print(f"[FATAL] {sp}: ç¼ºå°‘ GFF æˆ– Genomeã€‚", file=sys.stderr)
            write_match_table(match_path, [rows_out[k] for k in sorted(rows_out)])
            sys.exit(1)

        # è§„èŒƒåŒ–å·¥ä½œåŒºåœ¨æ¯ä¸ª worker ä¸­è¿›è¡Œï¼Œè¿™é‡Œåªåšè¦†ç›–åº¦æ£€æŸ¥
        cov, gcnt, fcnt, ocnt = coverage_details(gff, fa)
        print(f"  - {sp}: coverage = {cov:.1%}")
        print(f"    Â· GFF seqid: {gcnt}  Genome seqid: {fcnt}  Overlap: {ocnt}")

        rows_out[sp] = {
            "species": sp,
            "gff_file": str(gff),
            "genome_file": str(fa),
            "cov_frac": f"{cov:.6f}",
            "status": ("PASS_CHECK" if cov>=SEQID_COVERAGE_MIN else "FAIL_COVERAGE"),
            "pep_kept": "0",
            "cds_kept": "0",
            "note": "",
            "timestamp": datetime.datetime.now().isoformat(timespec="seconds")
        }

        if cov < SEQID_COVERAGE_MIN:
            print(f"[FATAL] {sp}: GFF vs genome è¦†ç›–ç‡ {cov:.1%} < {SEQID_COVERAGE_MIN:.0%}ã€‚", file=sys.stderr)
            write_match_table(match_path, [rows_out[k] for k in sorted(rows_out)])
            sys.exit(4)

        out_pep = Path(OUT_PEP_DIR)/f"{sp}.faa"
        out_cds = Path(OUT_CDS_DIR)/f"{sp}.fna"
        if RESUME_SKIP_PASS and sp in cache and cache[sp].get("status","").startswith("PASS") \
           and out_pep.exists() and out_pep.stat().st_size>0 and out_cds.exists() and out_cds.stat().st_size>0:
            rows_out[sp] = {**cache[sp]}
            print(f"  - {sp}: å‘ç°å†å²è¾“å‡ºä¸”çŠ¶æ€PASS â†’ è·³è¿‡æå–")
            continue

        pairs.append((sp, gff, fa, rows_out[sp]))

    if not pairs and not any((Path(OUT_PEP_DIR)/f"{sp}.faa").is_file() for sp in species):
        print("[FATAL] æ²¡æœ‰ç‰©ç§é€šè¿‡è¦†ç›–åº¦æ£€æŸ¥ï¼Œä¸”æ— å†å²è¾“å‡ºã€‚", file=sys.stderr)
        write_match_table(match_path, [rows_out[k] for k in sorted(rows_out)])
        sys.exit(2)

    # ============ å¹¶è¡Œè·‘ç‰©ç§ ============
    print(f"[STEP] ç‰©ç§å¹¶è¡Œå¤„ç†ï¼šworkers={N_WORKERS}, tasks={len(pairs)}")
    results: List[Dict[str, str]] = []
    failures: List[Tuple[str, str]] = []

    with Pool(processes=N_WORKERS) as pool:
        for status, payload in pool.imap_unordered(_worker, pairs):
            if status == "OK":
                results.append(payload)
            else:
                failures.append((payload.get("species","?"), payload.get("error","")))

    # å°†æˆåŠŸç»“æœå†™å…¥ rows_out
    for r in results:
        rows_out[r["species"]] = r
    # æŠ¥å‘Šå¤±è´¥ä½†ä¸å½±å“å…¶å®ƒç‰©ç§äº§å‡º
    if failures:
        print("\n[WARN] ä»¥ä¸‹ç‰©ç§å¤„ç†å¤±è´¥ï¼ˆå·²è·³è¿‡ï¼Œè¯¦è§é”™è¯¯ä¿¡æ¯ï¼‰ï¼š")
        for sp, err in failures:
            print(f"  - {sp}: {err}")
            # ä¿ç•™å¤±è´¥çŠ¶æ€
            if sp in rows_out:
                rows_out[sp]["status"] = "FAIL_PROCESS"
                rows_out[sp]["note"] = err[:200]

    # species.list åŸºäºå·²æœ‰ proteomes ç”Ÿæˆ
    ensure_dir(Path(LIST_DIR))
    existing = [p.stem for p in Path(OUT_PEP_DIR).glob("*.faa") if p.stat().st_size>0]
    Path(SPECIES_LIST).write_text("\n".join(sorted(set(existing))) + "\n", encoding="utf-8")

    # æœ€ç»ˆæ¸…ç†ä¸é˜²æ±¡æŸ“ï¼ˆTMP ç”±å¼€å…³æ§åˆ¶ï¼‰
    print("[STEP] æœ€ç»ˆæ¸…ç† & æ±¡æŸ“é˜²æŠ¤ ...")
    try:
        if CLEANUP_TMP_AT_END:
            if Path(TMP_DIR).exists():
                shutil.rmtree(TMP_DIR, ignore_errors=True)
                print(f"[CLEAN] tmp dir {TMP_DIR} removed.")
        else:
            print(f"[KEEP] æŒ‰å¼€å…³ä¿ç•™ä¸´æ—¶ç›®å½•ï¼š{TMP_DIR}")

        # ä¿æŒåŸæœ‰æ³¨é‡Šç›®å½•â€œä¸­é—´äº§ç‰©æ‰«é™¤â€ç­–ç•¥
        ann = Path(ANNOT_DIR)
        for bad in ann.glob("*"):
            if re.search(r'\.(clean4agat|agat_longest|keep_longest|fixloc|fixphase|agat.*)\.gff3(\.gz)?$', bad.name):
                try:
                    bad.unlink()
                    print(f"[CLEAN] removed stray middleware in annotation: {bad.name}")
                except Exception as e:
                    print(f"[WARN] cannot remove {bad}: {e}")
    except Exception as e:
        print(f"[WARN] final cleanup exception: {e}")

    # å†™ match è¡¨
    write_match_table(Path(MATCH_TSV), [rows_out[k] for k in sorted(rows_out)])
    print(f"[OUT] Proteomes: {OUT_PEP_DIR}/*.faa")
    print(f"[OUT] CDS      : {OUT_CDS_DIR}/*.fna")
    print(f"[OUT] Species  : {SPECIES_LIST}")
    print(f"[OUT] MatchTbl : {MATCH_TSV}")
    print("[ALL DONE] ğŸ‰")

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        # ä¸»è¿›ç¨‹å…œåº•æŠ¥é”™ï¼ˆä¾‹å¦‚ä¾èµ–ç¼ºå¤±ï¼‰
        print(str(e), file=sys.stderr)
        sys.exit(1)

